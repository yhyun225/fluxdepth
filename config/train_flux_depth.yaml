base_config:
- config/logging.yaml
- config/wandb.yaml
- config/dataset_depth/dataset_train.yaml
- config/dataset_depth/dataset_val.yaml
- config/dataset_depth/dataset_vis.yaml
- config/model_flux.yaml

mixed_precision: 'bf16'

pipeline:
  name: FluxDepthPipeline
  kwargs:
    scale_invariant: true
    shift_invariant: true
    prediction_type: depth
    default_processing_resolution: 1024

depth_normalization:
  type: scale_shift_depth
  clip: true
  norm_min: -1.0
  norm_max: 1.0
  min_max_quantile: 0.02

augmentation:
  lr_flip_p: 0.5

dataloader:
  num_workers: 2
  effective_batch_size: 64
  max_train_batch_size: 16
  seed: 2024  # to ensure continuity when resuming from checkpoint

trainer:
  name: FluxDepthTrainer
  init_seed: 2024  # use null to train w/o seeding
  save_period: 5000
  backup_period: 1000
  validation_period: 50000
  visualization_period: 500 #1000

# lora config
lora:
  rank: 64
  alpha: 32
  modules: ~
prompt_embeds_dir: /home/yhyun225/fluxdepth/prompt_embeds

# flux parameters
guidance_scale: 1.0

# GLOWD config
max_iter_glowd: 5000


gt_depth_type: depth_raw_norm
gt_mask_type: valid_mask_raw

max_epoch: 10000  # a large enough number
max_iter: 30000  # usually converges at around 20k

optimizer:
  name: Adam

loss:
  name: mse_loss
  kwargs:
    reduction: mean

lr: 1.0e-04
lr_scheduler:
  name: IterExponential
  kwargs:
    total_iter: 25000
    final_ratio: 0.01
    warmup_steps: 100

# Memory efficient training configurations
use_8bit_adam: True
allow_tf32: True
enable_gradient_checkpointing: True
enable_xformers_memory_efficient_attention: False

# Light setting for the in-training validation and visualization
validation:
  num_inference_steps: [1, 2, 4, 10]
  # ensemble_size: 1
  processing_res: 0
  match_input_res: false
  # resample_method: bilinear
  main_val_metric: abs_relative_difference
  main_val_metric_goal: minimize
  init_seed: 2024

eval:
  alignment: least_square
  align_max_res: null
  eval_metrics:
  - abs_relative_difference
  - squared_relative_difference
  - rmse_linear
  - rmse_log
  - log10
  - delta1_acc
  - delta2_acc
  - delta3_acc
  - i_rmse
  - silog_rmse
